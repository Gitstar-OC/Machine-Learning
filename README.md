# Machine-Learning-Specialization ( In Progress...)
- ### This course is beginner friendly along with some tough challenges for intermidiate engineers in machines. Don't worry if you dont know python! Just give this a try 😄👍. 
- ### This are my machine learning notes I have created in my learning journey of AI and Machine, This will surely help beginners and intermidiate people to learn and understand machine learning. Feel free to clone, and fork 🍴.

--- 
- #### I also have occasionally added questions like taught in classes  to help make sure you understand the content like this 🙂
**Question:**
Description of question 
- Option A 
- Option B
>  Answer: Solution will be given here, with some explaination in below lines

---

>  `B` for Beginner, `I` for intermediate level and `A` for Advanced!!

>  `BN` for Easy Notebook, `IN` for Intermidiate, and `EN` for Notebook Assignment to test what you have learned!!


### Start with the basics
- `B` [Overview of Machine Learning](Basics/Overview.md) 
- `B` [Applications of Machine Learning](Basics/Applications.md)
- `B` [What is machine learning](Basics/MachineLearning.md)

### Difference between supervised and unsupervised machine learning
- `B` [Supervised Machine Learning Part 1](Supervised%20Learning/SupervisedLearning1.md)
- `B` [Supervised Machine Learning Part 2](Supervised%20Learning/SupervisedLearning2.md)
- `B` [Unsupervised Machine Learning Part 1](Unsupervised%20Machine%20Learning/UnsupervisedLearning1.md)
- `B` [Unsupervised Machine Learning Part 2](Unsupervised%20Machine%20Learning/UnsupervisedLearning2.md)

### Jupyter Notebooks
- Jupyter notebook is a type of `IDE` in machine learning which provides to many options to the developers to create and run the code. Check the below file to get started! 
> **Note:** You should first clone the project and open it in your IDE or in jupyter notebook to understand it in depth. 
### [Jupyter Notebook](Jupyter%20Notebooks/JupyterNotebook.md)

- `N` [JupyterNotebook](Jupyter%20Notebooks/IntroductiontoJupyterNotebook.ipynb) (**Clone and Open it in your IDE**)

--- 

## Superwised Machine Learning
- ### Regression Model
  - `B` [Linear Regression Part 1](Supervised%20Learning/Regression%20Model/LinearRegressionP1.md)
  - `B` [Linear Regression Part 2](Supervised%20Learning/Regression%20Model/LinearRegressionP2.md)
  - `BN` [Linear Regression Model Representation](Jupyter%20Notebooks/ModelRepresentation.ipynb) (**Open it in your IDE like VS Code or Jupyter Notebook**)
  - `B` [Cost Function Formula](Supervised%20Learning/Regression%20Model/CostFunctionFormula.md)
  - `B` [Cost Function Intuition](Supervised%20Learning/Regression%20Model/CostFunctionIntuition.md)
  - `B` [Visualizing the cost function](Supervised%20Learning/Regression%20Model/VisualizingCostFunction.md)
  - `B` [Visualization Examples](Supervised%20Learning/Regression%20Model/VisualizationExamples.md)
  - `BN` [Cost Function Model Representation](Jupyter%20Notebooks/CostFunctionVisualization.ipynb) (**Open it in your IDE and run the whole code by _Shift+Enter_**)

- ### Training Model with Gradient Descent 
  - `B` [Gradient Descent](Supervised%20Learning/Gradient%20Descent/GradientDescent.md)
  - `B` [Implement Gradient Descent](Supervised%20Learning/Gradient%20Descent/ImplementGradientDescent.md)
  - `B` [Gradient Descent Intuition ](Supervised%20Learning/Gradient%20Descent/GradientDescentIntuition.md)
  - `B` [Learning Rate](Supervised%20Learning/Gradient%20Descent/LearningRate.md)
  - `B` [Gradient Descent with Linear Regression ](Supervised%20Learning/Gradient%20Descent/GradientDescentLinearRegression.md)
  - `B` [Running Gradient Descent](Supervised%20Learning/Gradient%20Descent/RunningGradientDescent.md)
  - `BN` [Gradient Descent Representation](Jupyter%20Notebooks/GradientDescentRepresentaion.ipynb) (**Open it in your IDE and run the whole code by _Shift+Enter_**)


- ###  Multiple Linear Regression
  - `B` [Multiple Features](Supervised%20Learning/Multiple%20Linear%20Regression/MultipleFeatures.md)
  - `B` [Vectorization Part 1](Supervised%20Learning/Multiple%20Linear%20Regression/VectorizationPart1.md)
  - `B` [Vectorization Part 2](Supervised%20Learning/Multiple%20Linear%20Regression/VectorizationPart2.md)
  - `BN` [Python, Numphy and Vectorization](Jupyter%20Notebooks/PythonNumpyAndVectorization.ipynb) (**Clone and Open it in your IDE**)
  - `B` [Gradient Descent for Multiple Linear Regression](Supervised%20Learning/Multiple%20Linear%20Regression/GradientDescentforMultipleLinearRegression.md) 
  - `BN` [Multiple Variable Linear Regression](Jupyter%20Notebooks/MultipleLinearRegression.ipynb) (**Clone and Open it in your IDE**)
 
- ### Gradient Descent in Practice
  - `B` [Feature Scaling Part 1](Supervised%20Learning/Gradient%20Descent%20in%20Practice/FeatureScalingPart1.md) 
  - `B` [Feature Scaling Part 2](Supervised%20Learning/Gradient%20Descent%20in%20Practice/FeatureScalingPart2.md) 
  - `B` [Checking Gradient Descent for Convergence](Supervised%20Learning/Gradient%20Descent%20in%20Practice/CheckingGradientDescentforConvergence.md) 
  - `B` [Choosing the Learning Rate](Supervised%20Learning/Gradient%20Descent%20in%20Practice/ChoosingtheLearningRate.md)
  - `BN` [Feature Scaling and Learning Rate](Jupyter%20Notebooks/FeatureScalingAndLearningRate.ipynb) (**Clone and Open it in your IDE**)
  - `B` [Feature Engineering](Supervised%20Learning/Gradient%20Descent%20in%20Practice/FeatureEngineering.md)
  - `B` [Polynomial Regression](Supervised%20Learning/Gradient%20Descent%20in%20Practice/PolynomialRegression.md) 
  - `BN` [Feature Engineering and Polynomial Regression](Jupyter%20Notebooks/FeatureEngineeringAndPolynomialRegression.ipynb)  (**Clone and Open it in your IDE**)
  - `BN` [Linear Regression with scikit-learn](Jupyter%20Notebooks/LRwithScikitLearn.ipynb) (**Clone and Open it in your IDE**)
- `T` [Linear Regression Test Notebook/Lab](LinearRegressionTest.ipynb) (**Clone and Open it in your IDE, follow all the instructions given and write the solution code**)
- > Don't take any pressure of it, hints and solution are given in the notebook as well

- ### Classification with Logistic Regression
  - [Motivations]() 
  - `N` [Classification]()
  - [Logistic Regression]()
  - `N` [Sigmoid Function and Logistic Regression]() 
  - [Decision Boundary]() 
  - `N` [Decision Boundary]() 

<!--

- ### Cost Function for Logistic Regression 
  - [Cost Function for Logistic Regression]() 
  - `N` [Logistic Loss]() 
  - [Simplified Cost Function for Logistic Regression]() 
  - `N` [Cost Function for Logistic Regression]() 

- ### Gradient Descent for Logistic Regression 
  - [Gradient Descent Implementation]() 
  - `N` [Gradient Descent for Logistic Regression]() 
  - `N` [Logistic Regression with Scikit-Learn]() 

- ### The Problem of Overfitting 
  - [The Problem of Overfitting]() 
  - [Addressing Overfiting]() 
  - `N` [Overfitting]() 
  - [Cost Function with Regularization]() 
  - [Regularized Linear Regression]() 
  - [Regularized Logistic Regression]()
  - `N` [Regularization]()

--- 

## Advanced Learning Algorithms
- [Advanced Learning Algorithms]() (For => What you will learn in this part)

- ### Neural Network Model
  - [Neural Network Layer]() 
  - [More Complex Neural Networks]() 
  - [Inference: Making Predictions]() 
  - `N` [Neurons and Layers]() 
  
- ### TensorFlow Implementation
  - [Inference in Code]() 
  - [Data in TensorFlow]() 
  - [Building a Neural Network]()
  - `N` [Coffee Roasting in TensorFlow]() 
  

- ### Neural Network Implementation in Python 
  - [Forward prop in single layer]() 
  - [General Implementation of Forward Propogation]() 
  - `N` [Coffee Roasting NumPy]()

- Speculations on Artificial General Intelligence (AGI)
  - [Is there a path to AGI?]() 

- ### Vectorization (Optional) 
  - [How Neural Network are Implemented Effciently]() 
  - [Matrix Multiplication]() 
  - [Matrix Multiplication Rules]() 
  - [Matrix Multiplication Code]() 

- ### Neural Network Training 
  - [TensorFlow Implementation]() 
  - [Training Details]() 

- ### Activation Functions 
  - [Alternatives to the Sigmoid Activation]() 
  - [Choosing Activation Functions]()
  - [Why do we need Activation Functions]() 
  - `N` [ReLU Activation]() 

- ### Multiclass Classification 
  - [Multiclass]() 
  - [Softmax]() 
  - [Neural Network with Softmax output]() 
  - [Improved Implementation of Softmax]() 
  - [Classification with Multiple Outputs]() (Optional)
  - `N` [Softmax]() 
  - `N` [Multiclass]() 

- ### Additional Neural Network Concepts
  - [Advanced Optimization]() 
  - [Additional Layer Types]() 

- ### Back Propogation (Optional) 
  - [What is a derivative?]() 
  - [Computation Graph]() 
  - [Larger Neural Network Example]() 
  - `N` [Derivatives]() 
  - `N` [Back Propogation]() 
  
- ###  Advice for Applying Machine Learning 
  - [Deciding What to try next]() 
  - [Evaluating a Model]() 
  - [Model Selection and Training / Cross Validatiion / Test Sets]() 
  - `N` [Model Evaluation and Selection]() 

- ### Bias and Variance 
  - [Diagnosing Bias and Variance]() 
  - [Regularization and Bias/Variance]() 
  - [Establishing a baseline level of Performance]() 
  - [Learning Curves]()
  - [Deciding what to try next revisited]() 
  - [Bias / Variance and Neural Networks]() 
  - `N` [Diagnosing Bias and Variance]() 

- ### Machine Learning Development Process 
  - [Iterative Loop of ML Development]() 
  - [Error Analysis]() 
  - [Adding Data]() 
  - [Transfer Learning: Using Data from a Different Task]() 
  - [Full Cycle of Machine Learning Project]() 
  - [Fairness, Bias, and Ethics]() 

- ### Skewed Datasets (Optional) 
  - [Error Metrics for Skewed Datasets]() 
  - [Trading off precision and recall]() 

- ### Decision Trees
  - [Decision Tree Model]() 
  - [Learning Process]() 
  
- ### Decision Tree Learning
  - [Measuring Purity]() 
  - [Choosing a Split: Information Gain]() 
  - [Putting it Together]() 
  - [Using One-hot Encoding of Categorial Features]() 
  - [Continuous Valued Features]() 
  - [Regression Trees]() (Optional) 
  - `N` [Decision Trees]() 

  
- ### Tree Ensembles 
  - [Using Multiple Decison Trees]() 
  - [Sampling with Replacement]() 
  - [Random Forest Algorithm]() 
  - [XGBoost]() 
  - [When to Use Decision Trees]()
  - `N` [Tree Ensebles]()

--- 

## Unsupervised Machine Learning, Recommender Systems, Reinforcement Learning
- [Unsupervised Machine Learning]() (For => What you will learn in this part)

- ### Clustering
  - [What is Clustering]() 
  - [K-means Intuition]() 
  - [K-means Algorithm]() 
  - [Optimization Objective]()
  - [Initializing K-means]() 
  - [Choosing Numbers of clusters]() 

- ### Anomaly Detection 
  - [Finding Unusual Events]() 
  - [Gaussian (normal) distribution]() 
  - [Anomoly Detection Algoritm]() 
  - [Developing and Evaluating an Anomaly Detection System]() 
  - [Choosing What Features to Use]() 

- ### Collaborative Filtering 
  - [Making Recommendations]() 
  - [Using Per-Item Features]() 
  - [Collaborative Filtering Algorithm]() 
  - [Binary Labels: Favs, Likes and clicks]() 

- ### Recommender Systems Implementation Detail 
  - [Mean Normalization]() 
  - [TensorFlow Implementation of Collaborative Filtering]() 
  - [Finding Relatd Items]() 

- ### Content - Based Filtering 
  - [Collaborative Filtering vs Content-Based Filtering]() 
  - [Deep Learning for Content-Based Filtering]()
  - [Recommending from a Large Catalogue]() 
  - [Ethical Use of Recommender Systems]() 
  - [TensorFlow Implementations fo Content-Based Filtering]() 

- ### Principal Componenet Analysis (Optional) 
  - [Reducing Number of Features]()
  - [PCA algoritm]()
  - [PCA in Code]() 
  - `N` [PCA and Data Visualization]()

- ### Reinforcement Learning Introduction 
  - [What is Reinforcement Learning?]() 
  - [Mars Rover Example]() 
  - [The Return in Reinforcement Learning]() 
  - [Making Decisions: Policies in Reinforcement Learning]() 
  - [Review of Key Concepts]() 

- ### State - Action Value Function 
  - [State-action Value Function Definition]() 
  - [State-action Value Function Example]() 
  - `N` [State-action Value Function]() 
  - [Bellman Equation]() 
  - [Random (stochastic) Environment]() (Optional) 
  

- ### Continuous State Spaces 
  - [Example of Continuous State Space Applications]()
  - [Lunar Lander]() 
  - [Learning the State-value Function]() 
  - [Algorithm Refinement: Improved Neural Network Architecture]()
  - [Algorithm Refinement: E- Greedy Policy]() 
  - [Algorithm Refinement: Mini-Batch and Soft Updates]() (Optional)
  - [The State of Reinforcement Learning]() 


  Welldone Champ
 # Give it a ⭐ if you liked this!

