The very first step of the K means clustering algorithm, was to choose random locations as the initial guesses for the cluster centroids mu one through mu K. But how do you actually take that random guess. Let's take a look at that in this video, as well as how you can take multiple attempts at the initial guesses with mu one through mu K. That will result in your finding a better set of clusters. Let's take a look, here again is the K means algorithm and in this video let's take a look at how you can implement this first step. When running K means you should pretty much always choose the number of cluster centroids K to be lessened to training examples m. It doesn't really make sense to have K greater than m because then there won't even be enough training examples to have at least one training example per cluster centroids. So in our earlier example we had K equals two and m equals 30.
Play video starting at :1:4 and follow transcript1:04
In order to choose the cluster centroids, the most common way is to randomly pick K training examples. So here is a training set where if I were to randomly pick two training examples, maybe I end up picking this one and this one. And then we would set new one through mu K equal to these K training examples. So I might initialize my red cluster centroid here, and initialize my blue cluster centroids over here, in the example where K was equal to two. And it turns out that if this was your random initialization and you were to run K means you pray end up with K means deciding that these are the two classes in the data set. Notes that this method of initializing the cost of central is a little bit different than what I had used in the illustration in the earlier videos. Where I was initializing the cluster centroids mu one and mu two to be just random points rather than sitting on top of specific training examples.
Play video starting at :2:20 and follow transcript2:20
I've done that to make the illustrations clearer in the earlier videos. But what I'm showing in this slide is actually a much more commonly used way of initializing the clusters centroids.
Play video starting at :2:33 and follow transcript2:33
Now with this method there is a chance that you end up with an initialization of the cluster centroids where the red cross is here and maybe the blue cross is here. And depending on how you choose the random initial central centroids K-means will end up picking a difference set of causes for your data set. Let's look at a slightly more complex example, where we're going to look at this data set and try to find three clusters so k equals three in this data.
Play video starting at :3:7 and follow transcript3:07
If you were to run K means with one random initialization of the cluster centroid, you may get this result up here and this looks like a pretty good choice. Pretty good clustering of the data into three different clusters. But with a different initialization, say you had happened to initialize two of the cluster centroids within this group of points. And one within this group of points, after running k means you might end up with this clustering, which doesn't look as good. And this turns out to be a local optima, in which K-means is trying to minimize the distortion cost function, that cost function J of C one through CM and mu one through mu K that you saw in the last video. But with this less fortunate choice of random initialization, it had just happened to get stuck in a local minimum.
Play video starting at :4:9 and follow transcript4:09
And here's another example of a local minimum, where a different random initialization course came in to find this clustering of the data into three clusters, which again doesn't seem as good as the one that you saw up here on top.
Play video starting at :4:27 and follow transcript4:27
So if you want to give k means multiple shots at finding the best local optimum. If you want to try multiple random initialization, so give it a better chance of finding this good clustering up on top. One other thing you could do with the K-means algorithm is to run it multiple times and then to try to find the best local optima.
Play video starting at :4:51 and follow transcript4:51
And it turns out that if you were to run k means three times say, and end up with these three distinct clusterings. Then one way to choose between these three solutions, is to compute the cost function J for all three of these solutions, all three of these choices of clusters found by k means. And then to pick one of these three according to which one of them gives you the lowest value for the cost function J.
Play video starting at :5:23 and follow transcript5:23
And in fact, if you look at this grouping of clusters up here, this green cross has relatively small square distances, all the green dots. The red cross is relatively small distance and red dots and similarly the blue cross. And so the cost function J will be relatively small for this example on top. But here, the blue cross has larger distances to all of the blue dots. And here the red cross has larger distances to all of the red dots, which is why the cost function J, for these examples down below would be larger. Which is why if you pick from these three options, the one with the smallest distortion of the smallest cost function J. You end up selecting this choice of the cluster centroids. So let me write this out more formally into an algorithm, and wish you would run K-means multiple times using different random initialization.
Play video starting at :6:26 and follow transcript6:26
Here's the algorithm, if you want to use 100 random initialization for K-means, then you would run 100 times randomly initialized K-means using the method that you saw earlier in this video.
Play video starting at :6:44 and follow transcript6:44
Pick K training examples and let the cluster centroids initially be the locations of those K training examples. Using that random initialization, run the K-means algorithm to convergence. And that will give you a choice of cluster assignments and cluster centroids. And then finally, you would compute the distortion compute the cost function as follows.
Play video starting at :7:12 and follow transcript7:12
After doing this, say 100 times, you would finally pick the set of clusters, that gave the lowest cost. And it turns out that if you do this will often give you a much better set of clusters, with a much lower distortion function than if you were to run K means only a single time. I plugged in the number up here as 100. When I'm using this method, doing this somewhere between say 50 to 1000 times would be pretty common. Where, if you run this procedure a lot more than 1000 times, it tends to get computational expensive. And you tend to have diminishing returns when you run it a lot of times. Whereas trying at least maybe 50 or 100 random initializations, will often give you a much better result than if you only had one shot at picking a good random initialization.
Play video starting at :8:10 and follow transcript8:10
But with this technique you are much more likely to end up with this good choice of clusters on top. And these less superior local minima down at the bottom. So that's it, when I'm using the K means algorithm myself, I will almost always use more than one random initialization. Because it just causes K means to do a much better job minimizing the distortion cost function and finding a much better choice for the cluster centroids.
Play video starting at :8:37 and follow transcript8:37
Before we wrap up our discussion of K means, there's just one more video in which I hope to discuss with you. The question of how do you choose the number of clusters centroids? How do you choose the value of K? Let's go on to the next video to take a look at that.