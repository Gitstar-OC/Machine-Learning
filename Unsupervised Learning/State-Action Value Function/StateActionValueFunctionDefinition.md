when we start to develop reinforcement learning hours later this week, you see that there's a key quantity that reinforcement learning algorithm will try to compute and that's called the state action value function. Let's take a look at what this function is. The state action value function is a function typically denoted by the letter uppercase Q. And it's a function of a state you might be in as well as the action you might choose to take in that state and Q of s,a. Will give a number that equals the return. If you start in that state. S and take the action A just once and after taking action A once you then behave optimally after that. So after that you take whatever actions will result in the highest possible return. Now you might be thinking there's something a little bit strange about this definition because how do we know what is the optimal behavior? And if we knew what the optimal behavior, if we already knew what's the best action to take in every state, why do we still need to compute Q of SA. Because we already have the optimal policy. So I do want to acknowledge that there's something a little bit strange about this definition. There's almost something a little bit circular about this definition, but rest assured When we look at specific reinforcement learning algorithms later will resolve this slightly circular definition and will come up with a way to compute the Q function even before we've come up with the optimal policy. But you see that in a later video. So don't worry about this for now. Let's look at an example we saw previously that this is a pretty good policy Go left from stage 2, 3 and four and go right from State five. It turns out that this is actually the optimal policy for the mars rover application When the discount factor gamma is 0.5, so Q of S. A will be equal to the total return If you start from say that take the action A and then behave optimally after that. Meaning take actions according to this policy. Shown over here, let's figure out what Q of s,a. Is for a few different states. Let's look at say Q of state too. And what if we take the action to go right well if you're in state two and you go right then you end up at state three And then after that you behave optimally you're going to go left from ST three and then go left from state to and then eventually you get the reward of 100. In this case, the rewards you get would be zero from state to zero when you get to stay three zero when you get back to state two and then 100 when you finally get to the terminal state one and so the return will be zero plus 0.5 times that plus 0.5 squared times ac plus 0.5 cubed times 100. And this turns out to be 12.5 And so Q of ST two of going right as equal to 12.5. Note that this passes, no judgment on whether going right is a good idea or not. It's actually not that good an idea from state two to go right, but it just faithfully reports out the return if you take action A and then behave optimally afterwards. Here's another example. If you're in state to and you were to go left, then the sequence of rewards you get will be zero when you're in state two followed by 100. And so the return is zero plus 0.5 times 100 that's equal to 50 in order to write down The values of Q(s,a). In this diagram, I'm going to write 12.5 here on the right to denote that this is Q of state two going to the right. And then when I write a little 50 here on the left to denote that this is Q of state two and going to the left just to take one more example What if we're in state four and we decide to go left. Well if you're in state four you go left, you get rewards zero and then you take action left here. So zero gain, take action left here, zero and then 100. So Q of four Left results in rewards zero because the first action is left and then because we followed the optimal policy afterwards You can reward 00 100. And so the return is zero plus 00.5 times that. Plus 4.5 squared times that plus 0.5 cubed times that. Which is therefore equal to 12.5. So Q4 left is 12.5. I'm going to write this here as 12.5. And it turns out if you were to carry out this exercise for all of the other states and all of the other actions, you end up with this being the Q of s,a. For different states and different actions And then finally at the Terminal State. Well it doesn't matter what you do, you just get that terminal reward 100 or 40. So just write down those terminal awards over here. So this is Q of s,a. For every state state one through six and for the two actions, action left and action right. Because the state action value function is almost always denoted by the letter Q. This is also often called the Q function. So the terms Q. Function and state action value function are used interchangeably and it tells you what are your returns or really what is the value? How good is it? Just take action A and state S and then behave optimally after that. Now it turns out that once you can compute the Q function this will give you a way to pick actions as well. Here's the policy and return. And here are the values Q of s,a. From the previous slide. You notice one interesting thing when you look at the different states which is that if you take state two taking the action left results in a,q. Value or state action value of 50 which is actually the best possible return you can get from that state. In state three two of s,a. for the action left also gives you that higher return in state four the action left gives you the return you want. And in state five is actually the action going to the right that gives you that higher return of 20. So it turns out that the best possible return from any state S. Is the largest value of Q of s,a A maximizing over A. Just to make sure this is clear what I'm saying is that in say state for There is Q of state four left which is 12.5 And q. of state four right, Which turns out to be 10. And the larger of these two values which is 12.5 Is the best possible return from that state four. In other words the highest return you can hope to get from State four is 12.5. And it's actually the larger of these two numbers 12.5 and 10. And moreover, if you want your Mars Rover to enjoy a return of 12.5 rather than say 10 then the action you should take is the action A. That gives you the larger value of Q of s,a. So the best possible action status is the action A. That actually maximizes Q, of s,a. So this might give you a hint for why computing Q, of s,a. Is an important part of the reinforcement learning algorithm that will build later. Namely if you have a way of computing Q of s,a. For every state and for every action then when you're in some state s all you have to do is look at the different actions A. And pick the action A. That maximizes Q of s,a. And so pi of s. S can just pick the action A. That gives the largest value of Q of s,a. And that will turn out to be a good action. In fact it turned out to be the optimal action. Another intuition about why this makes sense is Qof s,a. Is returned if you start in the state S and take the action A. And then behave optimally after that. So in order to earn the biggest possible return, what you really want is to take the action A. That results in the biggest total return. That's why if only we have a way of computing Q f s,a. For every state taking the action A that maximizes return under these circumstances seems like it's the best action to take in that state. Although this isn't something you need to know for this course, I want to mention also that if you look online or look at the reinforcement learning literature, sometimes you also see this Q function written as Q. Star instead of Q. And this Q function is sometimes also called the optimal Q function. These terms just refer to the Q function exactly as we've defined it. So if you look at the reinforcement learning literature and read about Q. Star or the Q function, that just means the state action value function that we've been talking about. But for the purposes of this course you don't need to worry about this. So to summarize if you can compute Q of s,a. For every state and every action, then that gives us a good way to compute the optimal policy pi of S. So that's the state action value function or the Q function. We'll talk later about how to come up with an algorithm to compute them despite the slightly circular aspect of the definition of the Q function. But first let's take a look at the next video at some specific examples of what these values Q of s,a. Actually look like