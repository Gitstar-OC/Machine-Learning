In the practice lab, you see how to implement content-based filtering in TensorFlow. What I'd like to do in this video is just set through of you a few of the key concepts in the code that you get to play with. Let's take a look. Recall that our code has started with a user network as well as a movie that's work. The way you can implement this in TensorFlow is, it's very similar to how we have previously implemented a neural network with a set of dense layers. We're going to use a sequential model. We then in this example have two dense layers with the number of hidden units specified here, and the final layer has 32 units and output's 32 numbers. Then for the movie network, I'm going to call it the item network, because the movies are the items here, this is what the code looks like. Once again, we have coupled dense hidden layers, followed by this layer, which outputs 32 numbers. For the hidden layers, we'll use our default choice of activation function, which is the relu activation function. Next, we need to tell TensorFlow Keras how to feed the user features or the item features, that is the movie features to the two neural networks. This is the syntax for doing so. That extracts out the input features for the user and then feeds it to the user and that we had defined up here to compute vu, the vector for the user. Then one additional step that turns out to make this algorithm work a bit better is at this line here, which normalizes the vector vu to have length one. This normalizes the length, also called the l2 norm, but basically the length of the vector vu to be equal to one. Then we do the same thing for the item network, for the movie network. This extract out the item features and feeds it to the item neural network that we defined up there This computes the movie vector vm. Then finally, the step also normalizes that vector to have length one. After having computed vu and vm, we then have to take the dot product between these two vectors. This is the syntax for doing so. Keras has a special layer type, notice we had here tf keras layers dense, here this is tf keras layers dot. It turns out that there's a special Keras layer, they just takes a dot product between two numbers. We're going to use that to take the dot product between the vectors vu and vm. This gives the output of the neural network. This gives the final prediction. Finally, to tell keras what are the inputs and outputs of the model, this line tells it that the overall model is a model with inputs being the user features and the movie or the item features and the output, this is output that we just defined up above. The cost function that we'll use to train this model is going to be the mean squared error cost function. These are the key code snippets for implementing content-based filtering as a neural network. You see the rest of the code in the practice lab but hopefully you'll be able to play with that and see how all these code snippets fit together into working TensorFlow implementation of a content-based filtering algorithm. It turns out that there's one other step that I didn't talk about previously, but if you do this, which is normalize the length of the vector vu, that makes the algorithm work a bit better. TensorFlows has this l2 normalized motion that normalizes the vector, is also called normalizing the l2 norm of the vector, hence the name of the function. That's it. Thanks for sticking with me through all this material on recommender systems, it is an exciting technology. I hope you enjoy playing with these ideas and codes in the practice labs for this week. That takes us to the lots of these videos on recommender systems and to the end of the next to final week for this specialization. I look forward to seeing you next week as well. We'll talk about the exciting technology of reinforcement learning. Hope you have fun with the quizzes and with the practice labs and I look forward to seeing you next week.